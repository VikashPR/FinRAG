{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from financerag.tasks import FinDER\n",
    "from financerag.retrieval import DenseRetrieval, SentenceTransformerEncoder\n",
    "from financerag.rerank import CrossEncoderReranker\n",
    "from financerag.tasks.BaseTask import BaseTask  # optional if you prefer static evaluate\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "CORPUS_PATH = \"/Users/vikashpr/Dev/Python/FinanceRAG/icaif-24-finance-rag-challenge/finder_corpus.jsonl/corpus.jsonl\"\n",
    "QUERY_PATH = \"/Users/vikashpr/Dev/Python/FinanceRAG/icaif-24-finance-rag-challenge/finder_queries.jsonl/queries.jsonl\"\n",
    "QRELS_PATH = \"/Users/vikashpr/Dev/Python/FinanceRAG/icaif-24-finance-rag-challenge/FinDER_qrels.tsv\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class LocalFinDER(FinDER):\n",
    "    def load_data(self):\n",
    "        # override BaseTask.load_data so it doesn't try HF\n",
    "        self.queries = {}\n",
    "        self.corpus = {}\n",
    "\n",
    "corpus = {\n",
    "    doc[\"_id\"]: {\"title\": doc.get(\"title\", \"\"), \"text\": doc.get(\"text\", \"\")}\n",
    "    for doc in load_jsonl(CORPUS_PATH)\n",
    "}\n",
    "queries = {q[\"_id\"]: q[\"text\"] for q in load_jsonl(QUERY_PATH)}\n",
    "\n",
    "finder_task = LocalFinDER()\n",
    "finder_task.corpus = corpus\n",
    "finder_task.queries = queries\n",
    "\n",
    "df = pd.read_csv(QRELS_PATH, sep=\"\\t\")\n",
    "qrels = df.groupby(\"query_id\").apply(lambda g: dict(zip(g[\"corpus_id\"], g[\"score\"]))).to_dict()\n",
    "\n",
    "print(\"Querry Result\", qrels)\n",
    "\n",
    "encoder = SentenceTransformerEncoder(\n",
    "    model_name_or_path=\"intfloat/e5-large-v2\",\n",
    "    query_prompt=\"query: \",\n",
    "    doc_prompt=\"passage: \",\n",
    ")\n",
    "retriever = DenseRetrieval(model=encoder)\n",
    "\n",
    "retrieval_result = finder_task.retrieve(retriever=retriever, top_k=200)\n",
    "\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    ")\n",
    "reranking_result = finder_task.rerank(\n",
    "    reranker=reranker,\n",
    "    results=retrieval_result,\n",
    "    top_k=100,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "ndcg, map_, recall, precision = finder_task.evaluate(\n",
    "    qrels=qrels,\n",
    "    results=reranking_result,\n",
    "    k_values=[1, 5, 10],\n",
    ")\n",
    "print(\"NDCG:\", ndcg)\n",
    "print(\"MAP:\", map_)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "finder_task.save_results(output_dir=\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enbedding Model: https://huggingface.co/intfloat/e5-large-v2\n",
    "\n",
    "This model has 24 layers and the embedding size is 1024.\n",
    "\n",
    "\n",
    "- Corpus: 13,867 documents (from finder_corpus.jsonl)\n",
    "- Queries: 218 queries (from finder_queries.jsonl)\n",
    "- Encoding: The model encodes all documents in 217 batches for the corpus and 4 batches for queries\n",
    "- Reranking: 675 batches of query-document pairs for cross-encoder reranking\n",
    "\n",
    "Batching is used for memory efficiency and computational performance:\n",
    "\n",
    "- Query Encoding (4 batches): Your 128 queries are encoded in batches to avoid GPU/CPU memory overflow\n",
    "- Corpus Encoding (217 batches): The ~13,900 documents are too large to encode at once, so they're processed in chunks of 64 documents (default batch size)\n",
    "- Reranking (675 batches): The cross-encoder processes query-document pairs in batches of 32 to compute relevance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
